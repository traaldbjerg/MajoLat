\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

Entanglement has long been one of the most puzzling features of quantum mechanics. It was first described in 1935 by Einstein, Podolsky and Rosen, who noticed that the Hilbert space nature of quantum states allowed some particles to be in some special superpositions that would connect two particles even at a distance, and that measuring one of the particles could determine the state of the other particle. They conjectured in their seminal paper that the properties of such states were so absurd that their theoretical existence had to imply that quantum mechanics had to be an incomplete theory \cite{einstein_can_1935}. Investigating the question, Bell published in 1964 a set of correlation inequalities that any local, hidden-variable model had to satisfy, and then showed that entangled states were capable of violating them \cite{bell_einstein_1964}. The philosophical ramifications of such a result, which implies that quantum mechanics (and thus our world) must be inherently non-local if the existence of entangled states was ever demonstrated experimentally, are profound.

The experimental confirmation of entanglement only came much later, when experiments showed that photon pairs could indeed violate Bell's inequalities, showing stronger correlations than classically possible \cite{aspect_experimental_1982}. This verification prompted new enthusiasm for the theory of entanglement and its ramifications in many fields, looking for deeper insight into the nature of the universe and advanced technological applications. One such field is quantum information, where entanglement has been the source of powerful technological prospects such as quantum cryptography, quantum key distribution or distributed quantum computing \cite{horodecki_quantum_2009}.

In 1999, Nielsen proved the conditions under which an entangled pure state shared between two quantum computers can be deterministically transformed into another if the computers can only communicate through a classical channel, a practical paradigm known as Local Operations and Classical Communications (LOCC) \cite{nielsen_conditions_1999}. This result showed that manipulating entangled states is theoretically feasible, and a few months later Vidal proved a generalization of his theorem, giving the maximal success probability of non-deterministic entanglement transformations \cite{vidal_entanglement_1999}. 

Interestingly, both of these impactful theorems are written in the language of majorization theory, which is a mathematical field that studies how to compare probability distributions based on how certain they are \cite{marshall_inequalities_2011}. Such concepts can be connected to information theory, thanks to a property of the Shannon entropy called Schur-concavity, which is not surprising as it also attempts to capture how uncertain a random variable is \cite{marshall_inequalities_2011, cover_elements_2006}. The majorization relation is not a total order, and so probability distributions do not have a \textit{more/less ordered than} relationship with every other probability distribution: we say that the majorization relation induces a \textit{preorder} on probability distributions. Such preorders sometimes admit an infimum (the meet) and supremum (the join), generating mathematical structures called lattices, which are well-studied in order theory \cite{davey_introduction_2002}. In 2002, Cicalese and Vaccaro showed that the majorization relation does admit such an infimum and supremum, introduced the majorization lattice, and showed some new properties of the Shannon entropy on these meet and join distributions known as \textit{supermodularity} and \textit{subadditivity} \cite{cicalese_supermodularity_2002}. In 2013, along with Gargano, they also used the majorization lattice to define an entropic distance for probability distributions \cite{cicalese_information_2013}.

The majorization lattice is a more sophisticated tool to study majorization relations and entropic inequalities. Taking inspiration from economics, recent years have seen the development of Quantum Resource Theories (QRT) such as the theories of quantum entanglement, quantum thermodynamics, quantum coherence, and many others \cite{chitambar_quantum_2019}. Several of those are majorization-based, and the majorization lattice was used to define special states, called the Optimal Common Resource (OCR) of a set of targets which is the state with the least amount of (entanglement) resource capable of reaching (by LOCC) all of the targets of the set, ensuring minimal waste of resource \cite{bosyk_optimal_2019}. A recent work at QuIC also found the best conversion protocol to produce such OCRs, and the optimality of the protocol was proven using the lattice \cite{deside_probabilistic_2024}.

The goal of this work was to explore the applications of the majorization lattice to the theory of quantum entanglement. Several avenues were explored, but the main idea that was explored was to study and quantify the incomparability between probability distributions. This was done because in the LOCC context, incomparability seems to enable more diversity in the set of reachable states, and is thus a desirable property for two states. This idea can be generalized to sets of states, enabling a new volumic intuiton for the Shannon entropy, which can be used to select states in LOCC protocols based on their redundancy relative to a set of states.

This master thesis is separated in two parts. The first part provides an overview of the litterature. First, in Chapter \ref{chap:majorization}, a theoretical explanation of majorization theory and the majorization lattice is given. Then, Chapter \ref{chap:quantum_information} provides a review of quantum information, going from classical information and the Shannon entropy to Nielsen's LOCC reachability theorems, which are the main connection point to majorization theory.

The second part of this manuscript goes over new results found during this master thesis. Chapter \ref{chap:alternative} proves some of the most important mathematical properties of entropy we use in the following chapters. We go over a more general proof of subadditivity for a broader class of functions than only the Shannon entropy, which were achieved by using a concatenation-based technique to find an underlying majorization relation (which we call a \textit{majorization precursor}). A similar majorization precursor, which seems to hold numerically, is also conjectured for supermodularity. Then, Chapter \ref{chap:criteria} quickly goes over an improvement to entropic separability criteria based on the meet of two distributions (which might be of interest in some experimental setups). However, the improvement over the previous criterion directly depends on how incomparable the two vectors are, motivating an attempt to quantify the incomparability between pairs of distributions. In an attempt to do this, we go over definitions of new entropic quantities which we call incomparability monotones, quantifying how incomparable two probability distributions are by taking inspiration from QRTs, along with theorems showing their behavior under LOCC. Finally, the next logical step was to generalize the incomparability monotones to sets of states, which is done in Chapter \ref{chap:volume}. This was done by exploiting the inclusion-exclusion principle and a relation between the intersection of majorization cones and the cone of their join, taking inspiration from geometrical properties of majorization cones that have recently been studied in the field of quantum thermodynamics \cite{junior_geometric_2022}. With this new entropic inclusion-exclusion formula, we prove several properties which seem to indicate that the Shannon entropy behaves like the volume of majorization cones, and use this idea to quantify the uniqueness of a state relative to a bank of entangled states. Using these uniqueness measures, Resource-State Selection Strategies (RSSS) were proposed which, according to our numerical simulations, run out of entangled states capable of reaching successive entangled targets through LOCC slightly slower \textit{on average} than naive strategies based only on the entropy of individual states.
