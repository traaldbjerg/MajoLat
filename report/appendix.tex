\appendix
\chapter*{Appendices}
\addcontentsline{toc}{chapter}{Appendices}
\renewcommand{\thesection}{\Alph{section}}
\renewcommand{\theequation}{\thesection.\arabic{equation}}



\section{Cicalese and Vaccaro's original proof of supermodularity}

A REFORMULER This section goes over the original proof of theorem \ref{th:shannon_supermodularity} for Shannon entropy, from Ref. \cite{cicalese_supermodularity_2002}. The proof is structured in two lemmas. Note that the convention $0 \log \frac{1}{0} = 0$ is used (an event that never happens does not contribute to entropy). We will first need the following definition.

\begin{appendix_definition}[Inversion point]
    Let $p, q \in \mathcal{P}^d$. We say that the index $i \in \{2, \ldots, n\}$ is an \emph{inversion point} for $p$ and $q$ if either
\begin{equation}
    \sum_{l=1}^{i} p_l < \sum_{l=1}^{i} q_l \quad \text{and} \quad \sum_{l=1}^{i-1} p_l > \sum_{l=1}^{i-1} q_l    
\end{equation}
or, vice versa, if
\begin{equation}
    \sum_{l=1}^{i} p_l > \sum_{l=1}^{i} q_l \quad \text{and} \quad \sum_{l=1}^{i-1} p_l < \sum_{l=1}^{i-1} q_l.
\end{equation}
\end{appendix_definition}

Let $2 \leq i_1 < i_2 < \cdots < i_k \leq n$ be all and the only inversion points for $p, q \in \mathcal{P}^d$. Moreover, let
\begin{equation}
    t = (t_1, \ldots, t_n) = p \wedge q
\end{equation}
and
\begin{equation}
    s = (s_1, \ldots, s_n) = \beta(p, q).
\end{equation}

The proof of the theorem will be an immediate consequence of the following two claims, where we assume $i_0 = 0$ and $i_{k+1} = n + 1$ for the sake of definiteness. 

\begin{appendix_lemma} \label{lem:shannon_1}
    For each inversion point $i_r$, $r = 0, \ldots, k$, we have
    \begin{equation}
        \sum_{l=i_r+1}^{i_{r+1}-1} p_l \log \frac{1}{p_l} + q_l \log \frac{1}{q_l} = \sum_{l=i_r+1}^{i_{r+1}-1} t_l \log \frac{1}{t_l}.
    \end{equation}
\end{appendix_lemma} 

\begin{proof} Let us assume, without loss of generality, that
    \begin{equation}
        \sum_{l=1}^{i_r} p_l > \sum_{l=1}^{i_r} q_l.
    \end{equation}

    Then
    \begin{equation}
        \sum_{l=1}^{i_r} s_l = \sum_{l=1}^{i_r} p_l
    \end{equation}
    and for all $s = i_r + 1, \ldots, i_{r+1} - 1$, we also have
    \begin{equation}
        s_s = \sum_{l=1}^{s} p_l - \sum_{l=1}^{s} q_l = p_s.
    \end{equation}

    Accordingly, we have
    \begin{equation}
        \sum_{l=1}^{i_r} t_l = \sum_{l=1}^{i_r} q_l
    \end{equation}

    and for all $s = i_r + 1, \ldots, i_{r+1} - 1$, we have
    \begin{equation}
        t_s = \sum_{l=0}^{s} q_l - \sum_{l=0}^{s} t_l = q_s. \qedhere
    \end{equation}
\end{proof}

As an immediate consequence of lemma \ref{lem:shannon_1}, we have
\begin{equation}
    \sum_{l=i_r+1}^{i_{r+1}-1} \left( p_l \log \frac{1}{p_l} + q_l \log \frac{1}{q_l} - t_l \log \frac{1}{t_l} \right) = 0.
\end{equation}

\begin{appendix_lemma} \label{lem:shannon_2}
    For each inversion point $i_r$, $r = 1, 2, \ldots, k$, we have
    \begin{equation}
        s_{i_r} \log \frac{1}{s_{i_r}} + t_{i_r} \log \frac{1}{t_{i_r}} > p_{i_r} \log \frac{1}{p_{i_r}} + q_{i_r} \log \frac{1}{q_{i_r}},
    \end{equation}
    where $p_{i_r}, q_{i_r}, s_{i_r}, t_{i_r} > 0$.
\end{appendix_lemma} 

\begin{proof}
    Let us write $p, q, s, t$ instead of $p_{i_r}, q_{i_r}, s_{i_r}, t_{i_r}$, respectively. Without loss of generality let us assume that for the inversion point $i_r$ it holds that
    \begin{equation}
        \sum_{l=1}^{i_r} p_l > \sum_{l=1}^{i_r} q_l \quad \text{and} \quad \sum_{l=1}^{i_r-1} p_l < \sum_{l=1}^{i_r-1} q_l.
    \end{equation}

    It follows that $s > p$ and $q > t$, so $s > t$. Moreover, it is not hard to see that $s + t = p + q$. Let $s = q + \Delta$, then $t = p - \Delta$ and $s + t = p + q$. Then we have
    \begin{align}
        s \log \frac{1}{s} + t \log \frac{1}{t} - p \log \frac{1}{p} - q \log \frac{1}{q}
        &= - (s \log s + t \log t) + p \log p + q \log q \\
        &= - (q + \Delta) \log (q + \Delta) - (p - \Delta) \log (p - \Delta) + p \log p + q \log q \\
        &= (q \log q + p \log p) - \left( (q + \Delta) \log (q + \Delta) + (p - \Delta) \log (p - \Delta) \right) \\
        &\geq (q + p + \Delta) \log \left( \frac{(q + p + \Delta)^2}{(q + \Delta)(p - \Delta)} \right) \quad \text{(by Jensen's inequality)} \\
        &= (q + p + \Delta) \log \left( \frac{s + t + \Delta}{s + \Delta} \cdot \frac{s + t + \Delta}{t - \Delta} \right) \\
        &= (q + p + \Delta) \log \left( \frac{(s + t + \Delta)^2}{(s + \Delta)(t - \Delta)} \right) > 0,
    \end{align}
    since $s > t$ and $\Delta < t$.

    The remaining case $s \leq t$ is completely symmetric, provided that we set $s = p + \Delta$ and $t = q - \Delta$. \qedhere
\end{proof}

We are now ready to prove theorem \ref{th:shannon_supermodularity}. 

\begin{proof}[Proof of theorem \ref{th:shannon_supermodularity}]
    We have
    \begin{align}
        H(s) + H(t) - H(p) - H(q)
        &= \sum_{l=1}^{k} \left( s_l \log \frac{1}{s_l} + t_l \log \frac{1}{t_l} - p_l \log \frac{1}{p_l} - q_l \log \frac{1}{q_l} \right) \\
        &= \sum_{r=1}^{k} \left( \sum_{l=i_r+1}^{i_{r+1}-1} \left( s_l \log \frac{1}{s_l} + t_l \log \frac{1}{t_l} \right)
        + s_{i_r} \log \frac{1}{s_{i_r}} + t_{i_r} \log \frac{1}{t_{i_r}} \right. \\
        &\quad \left. - p_{i_r} \log \frac{1}{p_{i_r}} - q_{i_r} \log \frac{1}{q_{i_r}} 
        - \sum_{l=i_r+1}^{i_{r+1}-1} \left( p_l \log \frac{1}{p_l} + q_l \log \frac{1}{q_l} \right) \right) \\
        &\quad + \sum_{l=1}^{i_1 - 1} \left( s_l \log \frac{1}{s_l} + t_l \log \frac{1}{t_l} 
        - p_l \log \frac{1}{p_l} - q_l \log \frac{1}{q_l} \right) \\
        &\quad + \sum_{l=i_k+1}^{n} \left( s_l \log \frac{1}{s_l} + t_l \log \frac{1}{t_l} 
        - p_l \log \frac{1}{p_l} - q_l \log \frac{1}{q_l} \right) \\
        &= \sum_{r=1}^{k} \left( s_{i_r} \log \frac{1}{s_{i_r}} + t_{i_r} \log \frac{1}{t_{i_r}} 
        - p_{i_r} \log \frac{1}{p_{i_r}} - q_{i_r} \log \frac{1}{q_{i_r}} \right) 
        \quad \text{(by lemma \ref{lem:shannon_1})} \\
        &> 0 \quad \text{(by lemma \ref{lem:shannon_2}).} \qedhere
    \end{align}
\end{proof}




\newpage

\section{Entropy of compositions of random variables} \label{app:shannon_compositions}

\setcounter{equation}{0}

This section is based on \cite[pp. 16--22]{cover_elements_2006}. In practice, we are often concerned with more than one process at a given time, and so defining the uncertainty of two variables $X$, $Y$ is interesting. Moreover, knowing how much measuring one variable reduces the uncertainty on the second one (and so how much information on the second process we gain) on average is valuable as well. These concepts are captured by \textit{joint entropy}, \textit{conditional entropy} and \textit{mutual information}.

\begin{appendix_definition}[Joint entropy]
    Let $X$ and $Y$ be random variables over alphabets $\mathcal{X}$ and $\mathcal{Y}$ with joint probability distribution $p(x, y)$. Then the joint entropy $H(X, Y)$ of $X$ and $Y$ is defined as
    \begin{align}
        H(X, Y) &= - \sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log p(x, y)\\
                &= - E[\log p(x, y)].
    \end{align}
\end{appendix_definition}

\begin{appendix_definition}[Conditional entropy]
    Let $X$ and $Y$ be random variables over alphabets $\mathcal{X}$ and $\mathcal{Y}$ with marginal probability distributions $p(x)$ and $p(y)$. Then the conditional entropy $H(X|Y)$ of $X$ knowing $Y$ is defined as
    \begin{align}
        H(X|Y) &= \sum_{y \in \mathcal{Y}} p(y) H(X|Y = y)\\
                 &= - \sum_{y \in \mathcal{Y}} p(y) \sum_{x \in \mathcal{X}} p(x|y) \log p(x|y)\\
                 &= - \sum_{y \in \mathcal{Y}} \sum_{x \in \mathcal{X}} p(x, y) \log p(x|y)\\
                 &= - E[\log p(x|y)].
    \end{align}
\end{appendix_definition}

It is interesting to note that these definitions are all very consistent with each other, as the vision of entropy being the expected value of the random variable $\log \frac{1}{p(x)}$ holds even for compositions of random variables. Before defining mutual information, we introduce another mathematical tool, the Kullback-Leibler divergence $D(p \parallel q)$ of two probability mass functions $p$ and $q$, sometimes called \textit{relative entropy}.

\begin{appendix_definition}[Kullback-Leibler divergence]
    Let $p$ and $q$ be two probability mass functions over an alphabet $\mathcal{X}$. Then the Kullback-Leibler divergence $D(p \parallel q)$ of $p$ and $q$ is defined as
    \begin{align}
        D(p \parallel q) &= \sum_{x \in \mathcal{X}} p(x) \frac{p(x)}{q(x)}\\
                &= E_p\left[\log \frac{p(x)}{q(x)}\right],
    \end{align}
    with conventions $0 \log \frac{0}{0} = 0$, and $p \log \frac{p}{0} = \infty$ and $0 \log \frac{0}{q} = 0$ by continuity arguments.
\end{appendix_definition}

This quantity can be understood as measuring how far from each other the two distributions $p$ and $q$ are. If they are identical, i.e. $p(x) = q(x) \: \forall x \in \mathcal{X}$, then all of the terms in the sum are $\log \frac{p(x)}{p(x)} = \log 1 = 0$ and the relative entropy is simply 0. It is important to note however that this quantity is not a proper notion of distance, as it is not symmetric in its arguments, and fails the triangular inequality. % fort similaire a Cover and Thomas, faudrait reformuler un peu
Nonetheless, this notion of distance is used to define mutual information between two random variables as the Kullback-Leibler divergence between the joint probability distribution and the product of the marginal probability distributions.

\begin{appendix_definition}[Mutual information]
    Let $X$ and $Y$ be random variables over alphabets $\mathcal{X}$ and $\mathcal{Y}$ with marginal probability distributions $p(x)$ and $p(y)$ and joint distribution $p(x, y)$. Then the mutual information $I(X, Y)$ of $X$ and $Y$ is defined as
    \begin{align}
        I(X, Y) &= D(p(x, y)||p(x)p(y))\\
                &= \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log \frac{p(x, y)}{p(x)p(y)}\\
                &= E_{p(x, y)}\left[\log \frac{p(x, y)}{p(x)p(y)}\right]\\
                &= E_{p(x, y)}[- \log p(x)] + E_{p(x, y)}[- \log p(y)] - E_{p(x, y)}[- \log p(x, y)]\\
                &= E_{p(x)}[- \log p(x)] + E_{p(y)}[- \log p(y)] - E_{p(x, y)}[- \log p(x, y)]\\
                &= H(X) + H(Y) - H(X, Y).
    \end{align}
\end{appendix_definition}

Again, this definition agrees with what we would expect intuitively from a notion of the information that two random variables contain about each other: if the variables are independent, and thus contain no information about each other whatsoever (as the outcome of one does not affect the other), then their joint distribution reduces to the product of the marginal distributions, and the relative entropy in the definition becomes 0. Then, the less independent the joint distribution becomes, the farther it becomes from the product distribution, and the more measuring the outcome of one variable can tell you about the outcome of the second. It is interesting to note that the mutual information is symmetric in its arguments.

To finish this small incursion into classical information theory, it turns out that Shannon entropy is actually the only function that satisfies all the properties that one would expect from a measure of information, such as (non-exhaustive list):

\begin{itemize}
    \item $H(X) \geq 0$.
    \item $H(X, Y) = H(Y, X)$.
    \item $H(X, Y) = H(X) + H(Y|X)$.
    \item \textbf{Subadditivity}\footnote{This subadditivity has nothing to do with the subadditivity on the lattice.}\textbf{:} $H(X, Y) \leq H(X) + H(Y)$ with equality iff $X$ and $Y$ are independent.
    \item $H(X|Y) \leq H(X)$, which directly implies that $I(X, Y) \geq 0$, with equality iff $X$ and $Y$ are independent.
\end{itemize}



\newpage

\section{Mixed state example}
\label{app:mixed_example}

\setcounter{equation}{0}

Let us take a qubit, and let us try to find the difference between the states $\rho$ and $\rho'$ from the coin-flip example in section \ref{sec:density_matrices}. In the computational basis, they are written

\begin{align}
    \rho &= 1/2 \ket{0}\bra{0} + 1/2 \ket{1}\bra{1} = \left(\begin{matrix}
        1/2 & 0\\
        0 & 1/2
    \end{matrix}\right),\\
    \rho' &= \ket{\psi}\bra{\psi} = \ket{+}\bra{+} = \frac{1}{\sqrt{2}}(\ket{0} + \ket{1})\frac{1}{\sqrt{2}}(\bra{0} + \bra{1}) = \left(\begin{matrix}
        1/2 & 1/2\\
        1/2 & 1/2
    \end{matrix}\right).
\end{align}

In both cases, let us measure if the qubit is in state $\ket{0}$ or $\ket{1}$. Those measurements are described by the operators $M_0 = \ket{0}\bra{0}$ and $M_1 = \ket{1}\bra{1}$, which are simple projective measurements. For $\rho$, we have

\begin{align}
    p(0) &= \tr\Big(\ket{0}\bra{0}\ket{0}\bra{0}\big(1/2 \ket{0}\bra{0} + 1/2 \ket{1}\bra{1}\big)\Big)\\
         &= \tr\Big(1/2 \ket{0}\bra{0}\ket{0}\bra{0} + 1/2 \ket{0}\bra{0}\ket{1}\bra{1}\Big)\\
         &= \tr(1/2 \ket{0}\bra{0}\ket{0}\bra{0})\\
         &= 1/2,
\end{align}
\begin{align}
    p(1) &= \tr\Big(\ket{1}\bra{1}\ket{1}\bra{1}\big(1/2 \ket{0}\bra{0} + 1/2 \ket{1}\bra{1}\big)\Big)\\
         &= \tr\Big(1/2 \ket{1}\bra{1}\ket{0}\bra{0} + 1/2 \ket{1}\bra{1}\ket{1}\bra{1}\Big)\\
         &= \tr(1/2 \ket{1}\bra{1}\ket{1}\bra{1})\\
         &= 1/2.
\end{align}

\noindent For $\rho'$, we have

\begin{align}
    p(0) &= \tr\Big(\ket{0}\bra{0}\ket{0}\bra{0}\big(1/2 \ket{0}\bra{0} + 1/2 \ket{0}\bra{1} + 1/2 \ket{1}\bra{0} 1/2 \ket{1}\bra{1}\big)\Big)\\
         &= \tr\Big(1/2 \ket{0}\bra{0}\ket{0}\bra{0} +1/2 \ket{0}\bra{0}\ket{0}\bra{1} + 1/2 \ket{0}\bra{0}\ket{1}\bra{0} + 1/2 \ket{0}\bra{0}\ket{1}\bra{1}\Big)\\
         &= \tr(1/2 \ket{0}\bra{0} + 1/2 \ket{0}\bra{1})\\
         &= 1/2,
\end{align}
\begin{align}
    p(1) &= \tr\Big(\ket{1}\bra{1}\ket{1}\bra{1}\big(1/2 \ket{0}\bra{0} + 1/2 \ket{0}\bra{1} + 1/2 \ket{1}\bra{0} 1/2 \ket{1}\bra{1}\big)\Big)\\
         &= \tr\Big(1/2 \ket{1}\bra{1}\ket{0}\bra{0} +1/2 \ket{1}\bra{1}\ket{0}\bra{1} + 1/2 \ket{1}\bra{1}\ket{1}\bra{0} + 1/2 \ket{1}\bra{1}\ket{1}\bra{1}\Big)\\
         &= \tr(1/2 \ket{1}\bra{0} + 1/2 \ket{1}\bra{1})\\
         &= 1/2.
\end{align}

These are the expected results, and the difference between the pure and the mixed state are not so clear in this case. However, something interesting happens if we now try to measure the qubit in the dual basis, i.e. measuring along operators $M_+ = \ket{+}\bra{+}$ and $M_- = \ket{-}\bra{-}$. The calculation is easier in the dual basis, in which $\rho' = \ket{+}\bra{+}$, and so we directly get $p(+) = 1$. In this basis, using $\ket{0} = \frac{1}{\sqrt{2}}(\ket{+} + \ket{-})$ and $\ket{1} = \frac{1}{\sqrt{2}}(\ket{+} - \ket{-})$, $\rho$ becomes

\begin{align}
    \rho &= 1/2 \ket{0}\bra{0} + 1/2 \ket{1}\bra{1}\\
               &= 1/2 (\frac{1}{\sqrt{2}}(\ket{+} + \ket{-}))(\frac{1}{\sqrt{2}}(\bra{+} + \bra{-})) + 1/2 (\frac{1}{\sqrt{2}}(\ket{+} - \ket{-}))(\frac{1}{\sqrt{2}}(\bra{+} - \bra{-}))\\
               &= 1/2 \ket{+}\bra{+} + 1/2 \ket{-}\bra{-}.
\end{align}

\noindent And so the probabilities associated to outcomes + and - are

\begin{align}
    p(+) &= \tr\Big(\ket{+}\bra{+}\ket{+}\bra{+}\big(1/2 (\ket{+}\bra{+}) + 1/2 (\ket{-}\bra{-})\big)\Big)\\
         &= \tr\Big(1/2 \ket{+}\bra{+}\ket{+}\bra{+} + 1/2 \ket{+}\bra{+}\ket{-}\bra{-}\Big)\\
         &= \tr(1/2 \ket{+}\bra{+}\ket{+}\bra{+})\\
         &= 1/2,\\
    p(-) &= \tr\Big(\ket{-}\bra{-}\ket{-}\bra{-}\big(1/2 \ket{+}\bra{+} + 1/2 \ket{-}\bra{-}\big)\Big)\\
         &= \tr\Big(1/2 \ket{-}\bra{-}\ket{-}\bra{-} + 1/2 \ket{-}\bra{-}\ket{-}\bra{-}\Big)\\
         &= \tr(1/2 \ket{-}\bra{-}\ket{-}\bra{-})\\
         &= 1/2.
\end{align}

This is interesting, because this example showcases a fundamental difference between a superposition and a classical mixture. An appropriate choice of basis can make the outcome of measuring a state in a superposition certain, whereas the outcome of a measurement on a mixed state is \textit{always} uncertain, no matter the choice of measurement operator.



\newpage

\section{Probabilistic entanglement transformations} \label{app:vidal}

\setcounter{equation}{0}

\subsection{Weak majorization}

This section is based on \cite[pp. 10--15]{marshall_inequalities_2011}. It is sometimes useful to relax a bit the conditions on the majorization relation, in order to compare vectors that do not have the same sum. Two complementary notions can be defined, \textit{submajorization} and \textit{supermajorization}.

\begin{appendix_definition}[Submajorization] \label{def:submajorization}
    Let $p, q \in \mathbb{R}^d$ be two vectors of dimension $d$. We say that $p$ is submajorized by $q$, written $p \prec_w q$, if and only if
    \begin{equation} \label{eq:submajorization}
            S^\downarrow_k (q) \geq S^\downarrow_k (p) \quad \forall k = 1,...,d.
    \end{equation}
If the dimensions of the vectors do not match, one can always append zeroes to the one of lower dimension and apply the definition on the enlarged vector.
\end{appendix_definition}

\begin{appendix_definition}[Supermajorization] \label{def:supermajorization}
    Let $p, q \in \mathbb{R}^d$ be two vectors of dimension $d$. We say that $p$ is supermajorized by $q$, written $p \prec^w q$, if and only if
    \begin{equation} \label{eq:supermajorization}
            S^\uparrow_k (q) \leq S^\uparrow_k (p) \quad \forall k = 1,...,d,
    \end{equation}
where $S^\uparrow_k(p)$ and $S^\uparrow_k (q)$ denote the $k^\text{th}$ non-decreasing cumulative sums\footnote{Notice that using the reverse ordering means that the first components of $p^\uparrow$ are the smallest components of $p$, and so the smallest components of $p$ should be greater than those of $q$ in order to be more uncertain. This is why the inequality in equation (\ref{eq:supermajorization}) is reversed.} of $p$ and $q$. If the dimensions of the vectors do not match, one can always append zeroes to the one of lower dimension and apply the definition on the enlarged vector.
\end{appendix_definition}

Essentially, these definitions are the same as majorization, but they remove the condition that $S_d(p) = S_d(q)$. This allows to compare objects that do not have the same 1-norm.



\subsection{Vidal's theorem} \label{sec:vidal}

This section is based on Refs. \cite{vidal_entanglement_1999} and \cite{nielsen_majorization_2001}. Until now, only deterministic entanglement transformations have been discussed, i.e. transformations that succeed with probability one. However, a more general notion of entanglement transformations is the class of \textit{probabilistic} entanglement transformations. In the context of LOCC, we will denote the class of LOCC transformations that succeed with probability $p$ by LOCC$_p$. As such, the class of deterministic LOCC transformations we have discussed until now is simply LOCC$_1$. Probabilistic transformations are powerful, because they can break the majorization condition of theorem \ref{th:nielsen}, meaning that such transformations can sometimes gain entanglement, even though the operations in themselves are not nonlocal in nature. While this may seem strange, the key lies in the word \textit{sometimes}: such a protocol has probability $p$ of succeeding, but when it fails with probability $1-p$, the resulting state is less entangled, and \textit{on average} the amount of entanglement is (at most) preserved. The following theorem gives the maximal success probability with which a state can be transformed into another.

\begin{appendix_theorem}[Vidal's theorem] \label{th:vidal}
    Let $\ket{\phi}$ and $\ket{\psi}$ be two pure states of a bipartite system $AB$. Then,
    \begin{equation}
        \ket{\psi} \reach{\text{LOCC}_p} \ket{\phi} \iff \lambda_\psi \prec^w p \lambda_\phi,
    \end{equation}
    where $\lambda_\psi$ and $\lambda_\phi$ are the Schmidt vectors of $\ket{\psi}$ and $\ket{\phi}$, respectively.
\end{appendix_theorem}

% ecrire la preuve ?

\noindent This time, a supermajorization relation appears. The maximal success probability can thus be acquired from the theorem by finding the limiting probability $p$. 

\begin{appendix_corollary}
    Probabilistic LOCC transformations cannot increase the Schmidt rank of a state, because if $\lambda_\psi$ has Schmidt rank $k$ and $\lambda_\phi$ has Schmidt rank $k' > k$, then supermajorization relation $k+1$ is only satisfied for $p=0$.
\end{appendix_corollary}





\newpage

\section{Proofs of monotonicity under bistochastic degradation of the reference} \label{app:q_monotonicity}

\setcounter{equation}{0}

We prove here theorems \ref{th:monotone_future_q} and \ref{th:monotone_past_q} from section \ref{sec:q_monotonicity}. Starting with the future incomparability function, if we can show that $E^+(p \parallel Dq) < E^+(p \parallel q)$ for any bistochastic matrix $D$, then $E^+$ would be a decreasing monotone, like we expect. We first need a preliminar lemma.

\begin{appendix_lemma} \label{lem:monotone_future_q}
    For any $p, q \in \mathcal{P}^d$, we have
    \begin{equation}
        p \vee q \succ p \vee q' \; \; \; \forall \: q' \prec q.
    \end{equation}
\end{appendix_lemma}

\begin{proof}
    Let $p, q \in \mathcal{P}^d$ and let $q'$ be any probability vector majorized by $q$.
    \begin{equation}
        p \vee q = p \vee (q \vee q') = p \vee (q' \vee q) = (p \vee q') \vee q \succ p \vee q'.  \qedhere
    \end{equation} 
\end{proof}

\noindent This lemma is all we need to prove theorem \ref{lem:monotone_future_q} concerning the monotonicity of $E^+$ under bistochastic degradation of $q$.

\begin{proof}[Proof of theorem \ref{th:monotone_future_q}]
    Let us show that the maximum value of $E^+(p \parallel q')$ (with $q' \prec q$) is realized for $q'= q$.
    \begin{align}
        \max_{q' \prec q} E^+ (p \parallel q') &= \max_{q' \prec q} d(p, p \vee q')\\
        &= \max_{q' \prec q} H(p) - H(p \vee q')\\
        \overset{\text{Lemma \ref{lem:monotone_future_q}}}&{=} H(p) - H(p \vee q)\\
        &= d(p, p \vee q)\\
        &= E^+ (p \parallel q),
    \end{align}
    and so the maximal value of $E^+(p \parallel Dq)$ is reached for the identity degradation. \qedhere
\end{proof}

Let us turn our attention to $E^-$. This time around, the expected property for $E^-$ does hold, and we do have monotonicity in $q$. If we can show that $E^-(p \parallel Dq) > E^+(p \parallel q)$ for any bistochastic matrix $D$, then $E^-$ would be an increasing monotone, like we expect. To prove it, we first need the following lemma.

\begin{appendix_lemma} \label{lem:monotone_past_q}
    For any $p, q \in \mathcal{P}^d$, we have
    \begin{equation}
        p \wedge q \succ p \wedge q' \; \; \; \forall \: q' \prec q.
    \end{equation}
\end{appendix_lemma}

\begin{proof}
    Let $p, q \in \mathcal{P}^d$, and let $q'$ be any probability vector majorized by $q$. We have
    \begin{equation}
        p \wedge q' = p \wedge (q \wedge q') = (p \wedge q) \wedge q' \prec p \wedge q. \qedhere
    \end{equation} 
\end{proof}

\noindent This lemma is all we need to prove theorem \ref{th:monotone_past_q} concerning the monotonicity of $E^-$ under bistochastic degradation of $q$.

\begin{proof}[Proof of theorem \ref{th:monotone_past_q}]
    Let $q'$ be a vector majorized by $q$, i.e. there exists a bistochastic matrix $D$ such that $q' = Dq$.
    \begin{align}
        \min_{q' \prec q} E^- (p \parallel q') &= \min_{q' \prec q} d(p, p \wedge q')\\
        &= \min_{q' \prec q} H(p \wedge q') - H(p)\\
        \overset{\text{Lemma \ref{lem:monotone_past_q}}}&{=} H(p \wedge q) - H(p)\\
        &= d(p, p \wedge q)\\
        &= E^- (p \parallel q),
    \end{align}
    and so the minimal value of $E^-(p \parallel Dq)$ is reached for the identity degradation. \qedhere
\end{proof}




\newpage

\section{Proof of the basic properties of unique entropy} \label{app:unique_entropy_properties}

\setcounter{equation}{0}

We prove here properties \ref{prop:commutativity}, \ref{prop:empty}, \ref{prop:p_absorption} and \ref{prop:q_absorption} from section \ref{sec:unique_entropy_properties}. The commutativity property \ref{prop:commutativity} is immediate by commutativity of the join, and so the ordering of the bank does not matter. This is what we would expect for an inclusion-exclusion formula, as the order in which we remove intersections of sets does not matter as long as we perform the full sum. Because the ordering of the bank is arbitrary, we can always make any vector the last vector of the bank, which is useful for proving properties \ref{prop:p_absorption}, \ref{prop:q_absorption} and \ref{prop:q_monotonicity} as showing the property for $i = k$ is sufficient.

Property \ref{prop:empty} is immediate as well, because the join of the certain distribution $\overline{\delta}_d$ with any other vector remains the certain distribution. As such, all of the terms in equation (\ref{eq:unique_entropy}) are the entropy of $\overline{\delta}_d$, which is 0.  The key insight needed to prove the other properties is given by lemma \ref{lem:induction_trick}, which we prove here.

\begin{proof}[Proof of Lemma \ref{lem:induction_trick}]
    Let $p, q_1, \dots, q_k, q_{k+1} \in \mathcal{P}^d$. Developing all of the terms in eq. (\ref{eq:unique_entropy}), and regrouping the combinations containing $q_{k+1}$, we get
    \begin{align}
        E^+(p \parallel q_1, \dots, q_k, q_{k+1}) &= H(p) - \Big[H(p \vee q_1) + \dots + H(p \vee q_k) + H(p \vee q_{k+1})\Big] \nonumber \\
                                                  &\quad + \Big[H(p \vee q_1 \vee q_2) + \dots + H(p \vee q_{k-1} \vee q_k) + H(p \vee q_1 \vee q_{k+1}) \nonumber \\
                                                  &\quad + \dots + H(p \vee q_k \vee q_{k+1})\Big] - \dots\nonumber \\
                                                  &\quad + (-1)^{k+1} H(p \vee \dots \vee q_k \vee q_{k+1})\\
                                                  &= \Big(H(p) - \big[H(p \vee q_1) + \dots + H(p \vee q_k)\big] + \big[H(p \vee q_1 \vee q_2)\nonumber \\
                                                  &\quad + \dots + H(p \vee q_{k-1} \vee q_k)\big] - \dots + (-1)^{k} H(p \vee \dots \vee q_{k-1} \vee q_{k})\Big) \nonumber \\
                                                  &\quad - \Big(H(p \vee q_{k+1}) - \big[H(p \vee q_1 \vee q_{k+1}) + \dots + H(p \vee q_k \vee q_{k+1})\big]\nonumber \\
                                                  &\quad + \big[H(p \vee q_1 \vee q_2 \vee q_{k+1}) + \dots + H(p \vee q_{k-1} \vee q_k \vee q_{k+1})\big]\nonumber \\
                                                  &\quad - \dots + (-1)^{k} H(p \vee \dots \vee q_k \vee q_{k+1})\Big). \label{eq:swapparoo_start}
    \end{align}
    By commutativity of the join, we can push $q_{k+1}$ next to $p$, and so equation (\ref{eq:swapparoo_start}) becomes
    \begin{align}
        E^+(p \parallel q_1, \dots, q_k, q_{k+1}) &= \Big(H(p) - \big[H(p \vee q_1) + \dots + H(p \vee q_k)\big] + \big[H(p \vee q_1 \vee q_2)\nonumber \\
                                                  &\quad + \dots + H(p \vee q_{k-1} \vee q_k)\big] - \dots + (-1)^{k} H(p \vee \dots \vee q_{k-1} \vee q_{k})\Big) \nonumber \\
                                                  &\quad - \Big(H(p \vee q_{k+1}) - \big[H(p \vee q_{k+1} \vee q_1) + \dots + H(p \vee q_{k+1} \vee q_k)\big]\nonumber \\
                                                  &\quad + \big[H(p \vee q_{k+1} \vee q_1 \vee q_2) + \dots + H(p \vee q_{k+1} \vee q_{k-1} \vee q_k)\big]\nonumber \\
                                                  &\quad - \dots + (-1)^{k} H(p \vee q_{k+1} \vee \dots \vee q_k)\Big).  \label{eq:swapparoo_done}
    \end{align}
    Comparing terms one by one, we can see that the second half of the expression is exactly the same as the first half (which is equal to $E^+(p \parallel q_1, \dots, q_k)$) if $p$ is replaced by $p \vee q_{k+1}$, and so equation (\ref{eq:swapparoo_done}) becomes
    \begin{equation}
        E^+(p \parallel q_1, \dots, q_k, q_{k+1}) = E^+(p \parallel q_1, \dots, q_k) - E^+(p \vee q_{k+1} \parallel q_1, \dots, q_k). \qedhere
    \end{equation}
\end{proof}

Such an expression lends itself very well to induction proofs. Let us now prove the remaining properties. For property \ref{prop:p_absorption}, we have the following.

\begin{appendix_lemma}[Absorption in $p$ of unique entropy]
    Let $p, q_1, \dots, q_k \in \mathcal{P}^d$. If $\exists i \leq k \: | \: p \succ q_i$, then
    \begin{equation}
        E^+(p \parallel q_1, \dots, q_k) = 0.
    \end{equation}
\end{appendix_lemma}

\begin{proof}
    The proof is immediate from lemma \ref{lem:induction_trick}. Because the ordering of the bank is arbitrary, let us first choose $i = k$, and so $p \succ q_k$. We have
    \begin{align}
        E^+(p \parallel q_1, \dots, q_k) &= E^+(p \parallel q_1, \dots, q_{k-1}) - E^+(\underbrace{p \vee q_k}_{= p} \parallel q_1, \dots, q_{k-1})\\
                                         &= 0.
    \end{align}
    The same result can be reached by comparing term by term the possible combinations of states in the bank\footnote{Exactly half of the terms in the sum contain $q_k$. Because each of those can be paired up with the same combination excluding $q_k$, we have terms of same absolute value but opposite sign, and so all of the terms cancel out.}. If $i \neq k$, commuting $q_i$ until it becomes the last vector of the bank and applying lemma \ref{lem:induction_trick} yields the same result. \qedhere
\end{proof}

This property is again quite intuitive, because if $p$ majorizes one of the vectors of the bank $q_i$, then $\mathcal{T}_+(p) \subseteq \mathcal{T}_+(q_i)$, and so $p$ holds no unique volume relative to the bank. The geometric interpretation of property \ref{prop:q_absorption} is very similar, and we have the following.

\begin{appendix_lemma}[Absorption in $q$ of unique entropy]
    Let $p, q_1, \dots, q_k, q_{k+1} \in \mathcal{P}^d$. If $\exists i \leq k \: | \: q_{k+1} \succ q_i$, then
    \begin{equation}
        E^+(p \parallel q_1, \dots, q_k, q_{k+1}) = E^+(p \parallel q_1, \dots, q_k).
    \end{equation}
\end{appendix_lemma}

\begin{proof}
    Let $p, q_1, \dots, q_k, q_{k+1} \in \mathcal{P}^d$.  Because the ordering of the bank is arbitrary, let us first choose $i = k$, and so $q_{k+1} \succ q_k$. Using lemma \ref{lem:induction_trick} twice in a row, we get
    \begin{align}
        E^+(p \parallel q_1, \dots, q_k, q_{k+1}) &= E^+(p \parallel q_1, \dots, q_k) - E^+(p \vee q_{k+1} \parallel q_1, \dots, q_k)\\
                                                  &= E^+(p \parallel q_1, \dots, q_k) - E^+(p \vee q_{k+1} \parallel q_1, \dots, q_{k-1})\nonumber\\
                                                  &\quad + E^+(p \vee \underbrace{q_{k+1} \vee q_k}_{= q_{k+1}} \parallel q_1, \dots, q_{k-1})\\
                                                  &= E^+(p \parallel q_1, \dots, q_k).
    \end{align}
    The same result can be reached by comparing term by term the possible combinations of states in the bank\footnote{Exactly half of the combinations containing $q_{k+1}$ also contain $q_k$. Because each of those can be paired up with the same combination excluding $q_k$, we have terms of same absolute value but opposite sign, and so all of the terms containing $q_{k+1}$ cancel out.}. If $i \neq k$, commuting $q_i$ until it becomes the $k^\text{th}$ vector of the bank and applying lemma \ref{lem:induction_trick} twice yields the same result. 
\end{proof}



\newpage

\section{Notions of measure theory} \label{app:measure_theory}

\setcounter{equation}{0}




\newpage

\section{Simulations of alternative filterings for LOCC state selection strategies} \label{app:alternative_filterings}

\setcounter{equation}{0}

